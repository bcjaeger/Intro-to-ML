---
title: "Introduction to Machine Learning (ML)"
author: "Byron C. Jaeger"
date: "Last updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
  
---

```{css, echo = FALSE}

.huge { font-size: 200% }
.large { font-size: 130% }
.small { font-size: 70% }

```


```{r setup, include=FALSE}

options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(
  echo=FALSE,
  fig.width=12, 
  fig.height=6, 
  dpi=72*5,
  cache=TRUE
)

library(diagram)
library(tidyverse)
library(gam)
library(gridExtra)
library(knitr)
library(kableExtra)
library(scales)
library(widgetframe)
library(magrittr)

```


# Hello! My name is Byron 

.left-column[

I like running, walking my dog, bad margaritas, Rocket League, and (of course) R.

I study <br/> machine learning, missing data, ambulatory blood pressure, and cardiovascular disease.

]

.right-column[

<img src="figs/run_R_kids.png" width="100%" align="right" />

]

---
class: middle, center

# Part 1: What is Machine Learning?

---
layout: true
background-image: url(figs/robot_hex.jpg)
background-position: 95% 5%
background-size: 18%

# What is ML?

---

.large[ML is well known, but common misconceptions abound:]

--

- .large[Deep learning is a synonym for ML]

--

- .large[Artificial intelligence is a synonym of ML]

--

- .large[Everything about ML is a black box.]

--

.large[If these are wrong, what is right?]

---

background-image: url(figs/Ai-vs-ML-vs-Deep-Learning.png)
background-size: 90%
background-position: 50% 50%

---

1. End-to-end data analysis

--

2. The bias-variance tradeoff principle

--

3. Internal validation of predictive algorithms

--

and one application to the Jackson Heart Study

--

*(Bonus)* Apply R packages from the `tidyverse` 

--

*(If there is time)* Communicate results from black-box methods

---
class: inverse, middle, center

# End-to-End Data Analysis

---

## **Data analysis** 

Usually oriented toward prediction or inference. 

--

### **Statistical inference**

- Forming judgments about the parameters of a population.

- Hypothesis driven (exploring is frowned upon)

- End-to-end analysis is rare

--

### **Predictive analytics** 

- Forming a decision rule that can assist with tasks that involve uncertainty.

- Performance driven (exploring is necessary)

- End-to-end analysis is common

---

class: middle, center

# End-to-end data analysis

```{r, out.width='90%'}

knitr::include_graphics(
  "Intro_to_Predictive_Analytics_files/data_analysis_flow.jpg"
)

```

---
class: middle, center

# Data product demo

### https://bcjaeger.shinyapps.io/DPE4NHTN_WebApp/

---
class: inverse, middle, center

# Bias-Variance Tradeoff
---
layout: true
background-image: url(hex_stickers/PNG/dplyr.png)
background-position: 95% 2.5%
background-size: 13%

# The bias-variance tradeoff

---

### What is it?

 You may recognize the terms bias and variance if you have some experience with statistics.

--

- Bias: difference between observed and predicted values (in training data)

- Variance: measure of dispersion in the distribution of predicted values.

These are correct (but not helpful) definitions.

---

### What is it?

You may recognize the terms bias and variance if you have some experience with statistics.

- Bias: how well does a model predict its training data?

- Variance: how complex is the model?

These are incorrect (but helpful) definitions.

--

*Fact:* Reducing bias generally increases variance. 

--

*Unfair question:* Does reducing bias reduce prediction error for new data?

--

Let's work through an example to demonstrate why this question is unfair

---

**Example:** Given a training set of 25 observations, develop a prediction rule that minimizes squared error for a testing set of 25,000 observations. 

```{r}

nsubs=25
ntst=25000
set.seed(3)
x=runif(nsubs,0,10)
ggdat=data.frame(y=x*(3+sin(pi*x/3))+rnorm(nsubs,sd=x),x=x)
xgrid=seq(min(x),max(x),length.out=100)
truth=data.frame(x=xgrid)%>%mutate(y=x*(3+sin(pi*x/3)))

ggplot(ggdat,aes(x=x,y=y))+geom_point(size=3)+
  labs(title='Simulated Data',x='X-value',y='Y-value')+
  geom_line(data=truth,aes(x=x,y=y),color='red',linetype=2)

```

---

```{r, cache=TRUE}

spars=seq(1.0,0.3,length.out=15)
mdl_cmplx=1:length(spars)
grid_plots=vector(mode='list',length=length(spars))
newdata=truth%>%mutate(y=y+rnorm(length(xgrid), sd=xgrid))
sqr<-function(x) x^2

mprf=data.frame(
  cmp=mdl_cmplx,
  trn=0,
  tst=0
)

i=1

for(i in mdl_cmplx){
  
  m=gam(y~s(x,spar=spars[i]),data=ggdat)
  ggdat$prd=predict(m)
  newdata$prd=predict(m,newdata=newdata)
  
  p1=ggplot(ggdat,aes(x=x,y=y))+geom_point(size=3)+
    geom_line(data=newdata,aes(x=x,y=prd),col='blue')+
    geom_line(data=truth,aes(x=x,y=y),linetype=2,col='red')+
    labs(title=paste("derivation data error:",format(round(sqrt(mean(sqr(
      ggdat$y-ggdat$prd))),3),nsmall=3), '\n Model AIC', format(round(
        AIC(m),2),nsmall=2)))
  
  mprf[i,c('trn','tst')]<-c(
    sqrt(mean(sqr(ggdat$y-ggdat$prd))),
    sqrt(mean(sqr(newdata$y-newdata$prd)))
  )
  
  p2=ggplot(mprf%>%
              tidyr::gather(variable,value,-cmp)%>%
              dplyr::filter(value>0)%>%
              dplyr::mutate(cmp=cmp-1),
            aes(x=cmp,y=value,col=variable))+
    geom_point(size=3)+
    theme(legend.position='')+
    scale_color_brewer(palette='Dark2')+
    labs(y='Model Error',x='Model Complexity',
         title='derivation error (blue) and \ntesting error (orange)')
  
  if(i>1) p2=p2+geom_line()

  grid_plots[[i]]=arrangeGrob(p1,p2,nrow=1)

}

simulated_data = ggdat

```

```{r, echo=TRUE, eval=FALSE}
# spar = 1 => least complex spline
model <- gam(y ~ s(x, spar=1), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[1]]) 
```
---
```{r, echo=TRUE, eval=FALSE}
# spar = 0.95 => a little more wiggle room
model <- gam(y ~ s(x, spar=0.95), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[2]]) 
```
---
```{r, echo=TRUE, eval=FALSE}
# spar = 0.90 => a little more wiggle room
model <- gam(y ~ s(x, spar=0.90), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[3]]) 
```
---

```{r, echo=TRUE, eval=FALSE}
# spar = 0.85 => more 
model <- gam(y ~ s(x, spar=0.85), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[4]]) 
```
---

```{r, echo=TRUE, eval=FALSE}
# spar = 0.80 => more 
model <- gam(y ~ s(x, spar=0.80), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[5]]) 
```
---
```{r, echo=TRUE, eval=FALSE}
# spar = 0.75 => more 
model <- gam(y ~ s(x, spar=0.75), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[6]]) 
```
---

```{r, echo=TRUE, eval=FALSE}
# spar = 0.70 => more
model <- gam(y ~ s(x, spar=0.70), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[7]]) 
```
---
```{r, echo=TRUE, eval=FALSE}
# spar = 0.65 => more (too much?)
model <- gam(y ~ s(x, spar=0.65), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[8]]) 
```
---
```{r, echo=TRUE, eval=FALSE}
# spar = 0.60 => more (too much?)
model <- gam(y ~ s(x, spar=0.60), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[9]]) 
```
---

```{r, echo=TRUE, eval=FALSE}
# spar = 0.55 => more (too much?)
model <- gam(y ~ s(x, spar=0.55), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[10]]) 
```
---

```{r, echo=TRUE, eval=FALSE}
# spar = 0.50 => more (too too much??)
model <- gam(y ~ s(x, spar=0.50), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[11]]) 
```
---
```{r, echo=TRUE, eval=FALSE}
# spar = 0.45 => more (much too much!!)
model <- gam(y ~ s(x, spar=0.45), data=simulated_data)
```

```{r} 
grid.arrange(grid_plots[[12]]) 
```

