<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Introduction to Machine Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Byron C. Jaeger" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to Machine Learning
### Byron C. Jaeger
### Last updated: 2019-11-27

---


&lt;style type="text/css"&gt;

.huge { font-size: 200% }
.large { font-size: 130% }
.small { font-size: 70% }

&lt;/style&gt;





# Hello! My name is Byron 

.left-column[

I like running, walking my dog, bad margaritas, Rocket League, and (of course) R.

I study &lt;br/&gt; machine learning, missing data, ambulatory blood pressure, and cardiovascular disease.

Also, I'm a &lt;br/&gt;Biostatistician!

]

.right-column[

&lt;img src="figs/run_R_kids_markedup.png" width="100%" align="right" /&gt;

]

---
class: center
background-image: url(figs/scream_face.jpg)
background-position: 50% 70%
background-size: 30%


# Wait.

## Is this a math talk?


---
class: middle, center, inverse

# PART I

# What is Machine Learning?

---
layout: true 
class: center, middle

---

### Machine learning is well known, but common misconceptions abound...

---

### Artificial intelligence is the same thing as machine learning...?

---

### Machine learning is a type of deep learning...?

---

### Everything that involves machine learning involves a black box...?

---

### If these are wrong, what is right?

---
layout: false

# What is ML?

&lt;img src="figs/ai_ml_dl.png" width="100%" /&gt;

---
background-image: url(figs/ml_diagram.png)
background-size: 90%
background-position: 50% 50%

---
layout: true

# ML vs. Inference

---
background-image: url(figs/stats_inference.png)
background-size: 15%
background-position: 95% 5%

## Statistical Inference

- .large[Forming judgments about the parameters of a population.]

- .large[**Hypothesis** driven (exploring is frowned upon)]

- .large[End-point of analysis is **knowledge**]

--

### Inference `\(\Rightarrow\)` guidelines `\(\Rightarrow\)` decisions `\(\Rightarrow\)` outcomes


---
background-image: url(figs/data_mining.jpg)
background-size: 20%
background-position: 95% 5%

## Supervised Learning 

- .large[Forming a prediction function to engage with uncertainty.]

- .large[**Performance** driven (exploring is necessary)]

- .large[End-point of analysis is a **prediction function**]

--

### Predictions `\(\Rightarrow\)` decisions `\(\Rightarrow\)` outcomes

---
layout: false
class: middle, center

&lt;img src="figs/data_analysis_flow.jpg" width="90%" /&gt;

---
class: middle, center

# [Data product demo](https://bcjaeger.shinyapps.io/DPE4NHTN_WebApp/)

---
class: middle, center, inverse

# PART II

# Core Principles of Machine Learning

---
class: middle, center

# Core principal 1:

# Bias-variance tradeoff

---
layout: true
background-image: url(figs/esl.jpg)
background-position: 96% 4%
background-size: 18%

# Core Principles of ML

### Bias-variance tradeoff


---


 You may recognize the terms bias and variance if you have some experience with statistics.

--

- **Bias:** expected difference between observed and predicted values (in derivation data)

- **Variance:** measure of dispersion in the distribution of predicted values.

These are correct (but not helpful) definitions.

---

You may recognize the terms bias and variance if you have some experience with statistics.

- **Bias:** how well does a model predict its derivation data?

- **Variance:** how complex is the model?

These are incorrect (but helpful) definitions.

---

**Example:** Suppose data are `\(\mathcal{D} = (Y,X)\)`, where `\(Y = f(X)\)` and `\(f\)` is an unknown function. 

![](index_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

---

**Example:** Suppose data are `\(\mathcal{D} = (Y,X)\)`, where `\(Y = f(X)\)` and `\(f\)` is an unknown function. 

![](index_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---

**Example:** Suppose data are `\(\mathcal{D} = (Y,X)\)`, where `\(Y = f(X)\)` and `\(f\)` is an unknown function. 

.pull-left[
**Workflow:** I give you a dataset with 25 observations:

```r
print(your_data)
```
]

.pull-right[

```
## # A tibble: 25 x 2
##        y     x
##    &lt;dbl&gt; &lt;dbl&gt;
##  1  8.05  1.68
##  2 41.7   8.08
##  3 11.2   3.85
##  4  7.83  3.28
##  5 17.5   6.02
##  6 15.9   6.04
##  7  3.90  1.25
##  8  6.60  2.95
##  9 20.6   5.78
## 10 22.1   6.31
## # ... with 15 more rows
```
]


---

**Example:** Suppose data are `\(\mathcal{D} = (Y,X)\)`, where `\(Y = f(X)\)` and `\(f\)` is an unknown function. 

.pull-left[
**Workflow:** I'm keeping a dataset away from you. It has 100 observations:

```r
print(my_data)
```
]

.pull-right[

```
## # A tibble: 100 x 2
##        x     y
##    &lt;dbl&gt; &lt;dbl&gt;
##  1 0.153 0.503
##  2 0.242 0.552
##  3 0.332 1.33 
##  4 0.421 1.61 
##  5 0.510 0.573
##  6 0.599 2.74 
##  7 0.688 2.51 
##  8 0.777 3.99 
##  9 0.866 2.74 
## 10 0.955 4.58 
## # ... with 90 more rows
```
]

---

**Example:** Suppose data are `\(\mathcal{D} = (Y,X)\)`, where `\(Y = f(X)\)` and `\(f\)` is an unknown function. 

.pull-left[
**Workflow:** I'll give you my `\(X\)` values, and then you'll try to predict what my `\(Y\)` values are 

```r
select(my_data, x)
```
]

.pull-right[

```
## # A tibble: 100 x 1
##        x
##    &lt;dbl&gt;
##  1 0.153
##  2 0.242
##  3 0.332
##  4 0.421
##  5 0.510
##  6 0.599
##  7 0.688
##  8 0.777
##  9 0.866
## 10 0.955
## # ... with 90 more rows
```
]

---

**Example:** Suppose data are `\(\mathcal{D} = (Y,X)\)`, where `\(Y = f(X)\)` and `\(f\)` is an unknown function. 

.pull-left[
**Workflow:** Let's say your predictions are the mean of your observed `\(Y\)` values, `$$\widehat{Y} = \frac{1}{25}\sum_{i=1}^{25} (Y_i^{\text{(your data)}})^2$$`.


```r
yobs &lt;- my_data$y
yobs[1:10]

yhat &lt;- mean(your_data$y)
yhat
```
]

.pull-right[

```
##            [,1]
##  [1,] 0.5034636
##  [2,] 0.5515679
##  [3,] 1.3320930
##  [4,] 1.6082271
##  [5,] 0.5734291
##  [6,] 2.7400121
##  [7,] 2.5085339
##  [8,] 3.9934786
##  [9,] 2.7431047
## [10,] 4.5792541
```

```
## [1] 14.41691
```
]

---

**Example:** Suppose data are `\(\mathcal{D} = (Y,X)\)`, where `\(Y = f(X)\)` and `\(f\)` is an unknown function. 

.pull-left[
**Workflow:** I will calculate how accurate your predictions are by computing `$$\sqrt{\frac{1}{100}\sum_{i=1}^{100} (Y_i^{\text{(my data)}}-\widehat{Y})^2}$$`

```r
# squared differences
sqr_diffs &lt;- (yobs - yhat)^2
# mean squared error
sqrt(mean(sqr_diffs))
```
]

.pull-right[

```
## [1] 11.70895
```
]

---

**Example:** Suppose data are `\(\mathcal{D} = (Y,X)\)`, where `\(Y = f(X)\)` and `\(f\)` is an unknown function. 

.pull-left[
**Workflow:** That's not a great mean squared error. Maybe we can do better if we develop a function `\(\widehat{f}(X)\)` that predicts `\(Y\)`.


```r
model &lt;- ???
```
]

.pull-right[

```
## [1] 11.70895
```
]

---




```r
# spar = 1 =&gt; least complex spline
model &lt;- gam(y ~ s(x, spar=1), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;
---

```r
# spar = 0.95 =&gt; a little more wiggle room
model &lt;- gam(y ~ s(x, spar=0.95), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;
---

```r
# spar = 0.90 =&gt; a little more wiggle room
model &lt;- gam(y ~ s(x, spar=0.90), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;
---


```r
# spar = 0.85 =&gt; more 
model &lt;- gam(y ~ s(x, spar=0.85), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;
---


```r
# spar = 0.80 =&gt; more 
model &lt;- gam(y ~ s(x, spar=0.80), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;
---

```r
# spar = 0.75 =&gt; more 
model &lt;- gam(y ~ s(x, spar=0.75), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-30-1.png)&lt;!-- --&gt;
---


```r
# spar = 0.70 =&gt; more
model &lt;- gam(y ~ s(x, spar=0.70), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-32-1.png)&lt;!-- --&gt;
---

```r
# spar = 0.65 =&gt; more (too much?)
model &lt;- gam(y ~ s(x, spar=0.65), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-34-1.png)&lt;!-- --&gt;
---

```r
# spar = 0.60 =&gt; more (too much?)
model &lt;- gam(y ~ s(x, spar=0.60), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-36-1.png)&lt;!-- --&gt;
---


```r
# spar = 0.55 =&gt; more (too much?)
model &lt;- gam(y ~ s(x, spar=0.55), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-38-1.png)&lt;!-- --&gt;
---


```r
# spar = 0.50 =&gt; more (too too much??)
model &lt;- gam(y ~ s(x, spar=0.50), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-40-1.png)&lt;!-- --&gt;
---

```r
# spar = 0.45 =&gt; more (much too much!!)
model &lt;- gam(y ~ s(x, spar=0.45), data=simulated_data)
```

![](index_files/figure-html/unnamed-chunk-42-1.png)&lt;!-- --&gt;

---
layout: false
background-image: url(gifs/cat_snow_gif.gif)
background-position: 50% 60%
background-size: 50%

# In summary...

---
class: inverse, center, middle

# Case Study

---
layout: true
background-image: url(figs/intermacs_logo.jpg)
background-position: 97% 3%
background-size: 15%

# Case Study

### INTERMACS

---

**What is it?**

- Interagency Registry for Mechanically Assisted Circulatory Support

- Contains data from patients with advanced heart failure receiving mechanical circulatory support (MCS). 

---

**Why use it?**

- Patients who elect to receive MCS are at risk for a number of adverse events related to the device, such as death, stroke, malfunction, and infection.

- Targeted medical intervention should be directed to patients at high risk for events. 
    
- But...**who** is at risk? and **what** are they at risk for?

---
layout: false
background-image: url(figs/intermacs_slides/Slide1.PNG)
background-position: 20% 50%
background-size: 120%

# Case Study

### INTERMACS

---
background-image: url(figs/intermacs_slides/Slide2.PNG)
background-position: 20% 50%
background-size: 120%

# Case Study

### INTERMACS

---
background-image: url(figs/intermacs_slides/Slide3.PNG)
background-position: 20% 50%
background-size: 120%

# Case Study

### INTERMACS

---



&lt;table class="table" style="margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Table 1: INTERMACS patient characteristics&lt;/caption&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="border-bottom:hidden" colspan="2"&gt;&lt;/th&gt;
&lt;th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="3"&gt;&lt;div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; "&gt;Survived 1 year&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Overall&lt;br&gt;(N = 18975) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Yes&lt;br&gt;(N = 18038) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Censored&lt;br&gt;(N = 191) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; No&lt;br&gt;(N = 746) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Age, years &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 56.9 (12.9) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 56.7 (12.9) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 52.7 (13.9) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 61.8 (11.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; White, n (%) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12717 (67.0) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12088 (67.0) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 114 (59.7) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 515 (69.0) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Implant year, n (%) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2008 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 436 (2.30) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 416 (2.31) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 6 (3.14) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 14 (1.88) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2009 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 828 (4.36) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 788 (4.37) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4 (2.09) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 36 (4.83) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2010 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1533 (8.08) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1462 (8.11) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13 (6.81) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 58 (7.77) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2011 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1793 (9.45) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1712 (9.49) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 10 (5.24) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 71 (9.52) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2012 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2147 (11.3) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2073 (11.5) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 8 (4.19) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 66 (8.85) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2013 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2552 (13.4) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2444 (13.5) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13 (6.81) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 95 (12.7) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2014 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2677 (14.1) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2541 (14.1) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 14 (7.33) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 122 (16.4) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2015 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2924 (15.4) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2781 (15.4) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12 (6.28) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 131 (17.6) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2016 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2587 (13.6) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2483 (13.8) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12 (6.28) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 92 (12.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2017 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1498 (7.89) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1338 (7.42) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 99 (51.8) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 61 (8.18) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style="padding: 0; border:0;" colspan="100%"&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style="padding: 0; border:0;" colspan="100%"&gt;
&lt;sup&gt;*&lt;/sup&gt; Table values are mean (standard deviation) and count (percent) for continuous and categorical variables, respectively.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;


---
class: center, middle

# Core principal 2:

# External validation

---
layout: true
background-image: url(figs/text_trio.jpg)
background-position: 96% 4%
background-size: 25%

# Core Principles of ML

---

### External validation

**Why is this a core principle?** 

Anyone can predict what they already know. The value of prediction is tied to anticipating *external* information.

**What is external information?**

Data that comes from a different 

- place and / or time.

- research organization and / or team

- instrument

- population

than the derivation data.

---

### External validation with INTERMACS

**Problem:** Develop a function `\(\widehat{f}(t):\)` <i class="fas  fa-user-circle "></i> `\(\rightarrow (0,1]\)` 

- `\(\widehat{f}(t)\)` predicts risk for *mortality* at or before `\(t\)`.

- `\(t\)` ranges from 1 week to 1 years post MCS surgery.

- <i class="fas  fa-user-circle "></i> is **new** patient data collected

    + prior to MCS surgery at a pre-implant visit (variables with prefix `m0`)
    
    + 1 week after MCS surgery at a follow-up visit (variables with prefix `m0_25`)

--

**Question:** How do we know `\(\widehat{f}(t)\)` is accurate?

- Bias-variace tradeoff `\(\Rightarrow\)` we need a relevant set of **new** data to test `\(\widehat{f}(t)\)`.

---
layout: false

&lt;table class="table" style="margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Table 1: INTERMACS patient characteristics&lt;/caption&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="border-bottom:hidden" colspan="2"&gt;&lt;/th&gt;
&lt;th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="3"&gt;&lt;div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; "&gt;Survived 1 year&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Overall&lt;br&gt;(N = 18975) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Yes&lt;br&gt;(N = 18038) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Censored&lt;br&gt;(N = 191) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; No&lt;br&gt;(N = 746) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Age, years &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 56.9 (12.9) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 56.7 (12.9) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 52.7 (13.9) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 61.8 (11.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; White, n (%) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12717 (67.0) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12088 (67.0) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 114 (59.7) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 515 (69.0) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Implant year, n (%) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2008 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 436 (2.30) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 416 (2.31) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 6 (3.14) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 14 (1.88) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2009 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 828 (4.36) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 788 (4.37) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4 (2.09) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 36 (4.83) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2010 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1533 (8.08) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1462 (8.11) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13 (6.81) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 58 (7.77) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2011 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1793 (9.45) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1712 (9.49) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 10 (5.24) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 71 (9.52) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2012 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2147 (11.3) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2073 (11.5) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 8 (4.19) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 66 (8.85) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2013 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2552 (13.4) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2444 (13.5) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13 (6.81) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 95 (12.7) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2014 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2677 (14.1) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2541 (14.1) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 14 (7.33) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 122 (16.4) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2015 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2924 (15.4) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2781 (15.4) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12 (6.28) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 131 (17.6) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;font-weight: bold;color: white !important;background-color: red !important;" indentlevel="1"&gt; 2016 &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: red !important;"&gt; 2587 (13.6) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: red !important;"&gt; 2483 (13.8) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: red !important;"&gt; 12 (6.28) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: red !important;"&gt; 92 (12.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;font-weight: bold;color: white !important;background-color: red !important;" indentlevel="1"&gt; 2017 &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: red !important;"&gt; 1498 (7.89) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: red !important;"&gt; 1338 (7.42) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: red !important;"&gt; 99 (51.8) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: red !important;"&gt; 61 (8.18) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style="padding: 0; border:0;" colspan="100%"&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style="padding: 0; border:0;" colspan="100%"&gt;
&lt;sup&gt;*&lt;/sup&gt; Table values are mean (standard deviation) and count (percent) for continuous and categorical variables, respectively.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;

---
layout: false

&lt;table class="table" style="margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Table 1: INTERMACS patient characteristics&lt;/caption&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="border-bottom:hidden" colspan="2"&gt;&lt;/th&gt;
&lt;th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="3"&gt;&lt;div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; "&gt;Survived 1 year&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Overall&lt;br&gt;(N = 18975) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Yes&lt;br&gt;(N = 18038) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Censored&lt;br&gt;(N = 191) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; No&lt;br&gt;(N = 746) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Age, years &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 56.9 (12.9) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 56.7 (12.9) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 52.7 (13.9) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 61.8 (11.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; White, n (%) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12717 (67.0) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12088 (67.0) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 114 (59.7) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 515 (69.0) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Implant year, n (%) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2008 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 436 (2.30) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 416 (2.31) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 6 (3.14) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 14 (1.88) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2009 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 828 (4.36) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 788 (4.37) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4 (2.09) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 36 (4.83) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2010 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1533 (8.08) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1462 (8.11) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13 (6.81) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 58 (7.77) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2011 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1793 (9.45) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1712 (9.49) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 10 (5.24) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 71 (9.52) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;font-weight: bold;color: white !important;background-color: green !important;" indentlevel="1"&gt; 2012 &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 2147 (11.3) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 2073 (11.5) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 8 (4.19) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 66 (8.85) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;font-weight: bold;color: white !important;background-color: green !important;" indentlevel="1"&gt; 2013 &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 2552 (13.4) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 2444 (13.5) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 13 (6.81) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 95 (12.7) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;font-weight: bold;color: white !important;background-color: green !important;" indentlevel="1"&gt; 2014 &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 2677 (14.1) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 2541 (14.1) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 14 (7.33) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 122 (16.4) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;font-weight: bold;color: white !important;background-color: green !important;" indentlevel="1"&gt; 2015 &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 2924 (15.4) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 2781 (15.4) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 12 (6.28) &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;color: white !important;background-color: green !important;"&gt; 131 (17.6) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2016 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2587 (13.6) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2483 (13.8) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12 (6.28) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 92 (12.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; 2017 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1498 (7.89) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1338 (7.42) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 99 (51.8) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 61 (8.18) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style="padding: 0; border:0;" colspan="100%"&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style="padding: 0; border:0;" colspan="100%"&gt;
&lt;sup&gt;*&lt;/sup&gt; Table values are mean (standard deviation) and count (percent) for continuous and categorical variables, respectively.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;

---
layout: true
background-image: url(figs/esl.jpg)
background-position: 96% 4%
background-size: 18%

# Core Principles of ML

---

### External validation with INTERMACS

**Problem:** Develop a function `\(\widehat{f}(t):\)` <i class="fas  fa-user-circle "></i> `\(\rightarrow (0,1]\)` 

- `\(\widehat{f}(t)\)` predicts risk for *mortality* at or before `\(t\)`.

- `\(t\)` ranges from 1 week to 1 years post MCS surgery.

- <i class="fas  fa-user-circle "></i> is **new** patient data collected

    + prior to MCS surgery at a pre-implant visit (variables with prefix `m0`)
    
    + 1 week after MCS surgery at a follow-up visit (variables with prefix `m0_25`)

**Question:** How do we know `\(\widehat{f}(t)\)` is accurate?

- Relevant set of **new** data to test `\(\widehat{f}(t)\)`: INTERMACS **data from 2016-2017**. <i class="fas  fa-check " style="color:green;"></i>

- Given `\(t\)`, validation outcomes `\(\in \left\{0, 1\right\} \Rightarrow\)` compare observed outcome to `\(\widehat{f}(t)\)`

---
layout: true
background-image: url(figs/scream_face.jpg)
background-position: 95% 2.5%
background-size: 13%

# Core Principles of ML


---

### Brier Score

The **Brier score** of a predicted probability `$$\widehat{f}(t) \in (0, 1]$$` for an observed event `$$Y(t) \in \left\{ 0, 1 \right\}$$` is `$$\left( \widehat{f}(t) -  Y(t) \right)^{2}.$$`

**Examples** 

- If `\(\widehat{f}(t) = 0.10\)` and `\(Y=0\)`, the **Brier score** is `\((0.10 - 0.00)^{2} = 0.01\)`.

- If `\(\widehat{f}(t) = 0.50\)`, the **Brier score** is always 0.25. Why?

---

### Brier Score at a given time


![](index_files/figure-html/unnamed-chunk-47-1.png)&lt;!-- --&gt;

---

### Brier Score at a given time

![](index_files/figure-html/unnamed-chunk-48-1.png)&lt;!-- --&gt;


---

### Brier Score at a given time

![](index_files/figure-html/unnamed-chunk-49-1.png)&lt;!-- --&gt;

---

### Brier Score at another given time


![](index_files/figure-html/unnamed-chunk-50-1.png)&lt;!-- --&gt;

---

### Brier Score at many given times


![](index_files/figure-html/unnamed-chunk-51-1.png)&lt;!-- --&gt;

---

### Integrated Brier Score

**Why?**

Integrating the Brier score over a range of times summarizes overall prediction accuracy. 

**How?**

Computing Brier scores at all potential times for one observation, i.e.,

`$$\text{ibs}(T) = \frac{1}{\text{T}} \int_{0}^{T} \left(\widehat{f}(t) - Y(t) \right)^{2} dt$$`

For a validation sample with `\(N\)` observations, 

`$$\text{IBS}(T) = \frac{1}{N} \sum_{i=1}^N \text{ibs}_i(T)$$`



---

### Scaled Brier Score

**Why?**

- IBS = 0 `\(\Rightarrow\)` perfect model. Great! What if IBS = 0.05?

- IBS = 0.25 `\(\Rightarrow\)` non-informative model, if the outcome has 50% prevalence.

- Max IBS is determined by the prevalence of the outcome.

**How?** 

If the outcome is present in 10% of the population, then the max IBS is 

`$$(0.1) \cdot (1–0.1)^{2} + (1–0.1) \cdot (0.1)^{2} = 0.090$$` 

So if our model gets an IBS of 0.05, we could scale it to be `\(\leq 1\)` using 

`$$1 - \frac{0.05}{0.090} = 0.444$$`

---

layout: true
background-image: url(figs/esl.jpg)
background-position: 96% 4%
background-size: 18%

# Core Principles of ML

---

### External validation with INTERMACS

**Problem:** Develop a function `\(\widehat{f}(t):\)` <i class="fas  fa-user-circle "></i> `\(\rightarrow (0,1]\)` 

- `\(\widehat{f}(t)\)` predicts risk for *mortality* at or before `\(t\)`.

- `\(t\)` ranges from 1 week to 1 years post MCS surgery.

- <i class="fas  fa-user-circle "></i> is **new** patient data collected

    + prior to MCS surgery at a pre-implant visit (variables with prefix `m0`)
    
    + 1 week after MCS surgery at a follow-up visit (variables with prefix `m0_25`)

**Question:** How do we know `\(\widehat{f}(t)\)` is accurate?

- Relevant set of **new** data to test `\(\widehat{f}(t)\)`: INTERMACS **data from 2016-2017**. <i class="fas  fa-check " style="color:green;"></i>

- Compare observed outcome to `\(\widehat{f}(t)\)` using the **scaled integrated Brier score**. <i class="fas  fa-check " style="color:green;"></i>



---
layout: true

# Case study

---
background-image: url(hex_stickers/PNG/dplyr.png)
background-position: 95% 2.5%
background-size: 13%

### ML workflow for INTERMACS

Make the derivation / validation sets

.pull-left[


```r
derivation &lt;- mcs %&gt;% 
  filter(
    m0_impl_yr &gt;= 2012,
    m0_impl_yr &lt;= 2015
  ) %&gt;% 
  select(-m0_impl_yr)

validation &lt;- mcs %&gt;% 
  filter(
    m0_impl_yr &gt;= 2016
  ) %&gt;% 
  select(-m0_impl_yr)

dim(derivation)

dim(validation)

dim(mcs)
```


]

.pull-right[


```
## [1] 10300   529
```

```
## [1] 4085  529
```

```
## [1] 18975   530
```


]

---
background-image: url(hex_stickers/PNG/recipes.png)
background-position: 95% 2.5%
background-size: 13%

### ML workflow for INTERMACS

The R package, `recipes`, lets you specify step-by-step instructions for data pre-processing.

The pre-processing steps applied here:

1. impute continuous variables to the mean,

2. impute categorical variables to the mode.


```r
reci &lt;- recipe(
  formula = time + status ~ ., 
  data = derivation
) %&gt;% 
  step_meanimpute(all_numeric()) %&gt;% 
  step_modeimpute(all_nominal()) %&gt;% 
  prep()
```

---
background-image: url(hex_stickers/PNG/recipes.png)
background-position: 95% 2.5%
background-size: 13%

### ML workflow for INTERMACS

The R package, `recipes`, lets you specify step-by-step instructions for data pre-processing.


```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          2
##  predictor        527
## 
## Training data contained 10300 data points and 10300 incomplete rows. 
## 
## Operations:
## 
## Mean Imputation for m0_age_deident, ... [trained]
## Mode Imputation for m0_admission_reason, ... [trained]
```

---
background-image: url(hex_stickers/PNG/recipes.png)
background-position: 95% 2.5%
background-size: 13%

### ML workflow for INTERMACS

`.derivation` and `.validation` represent pre-processed versions of `derivation` and `validation`. (In this case, pre-processed = imputed).


```r
# pre-processed derivation / validation data
.derivation &lt;- juice(reci)
.validation &lt;- bake(reci, new_data = validation)

dim(na.omit(derivation))
```

```
## [1]   0 529
```

```r
dim(na.omit(.derivation))
```

```
## [1] 10300   529
```


---
background-image: url(hex_stickers/PNG/recipes.png)
background-position: 95% 2.5%
background-size: 13%

### ML workflow for INTERMACS

`btimes` is a sequence comprising all of the times we will evaluate predicted risk for mortality.


```r
# times to evaluate predictions
btimes &lt;- seq(min(validation$time), 12, length.out = 500)

btimes[1:10]
```

```
##  [1] 0.1971293 0.2207824 0.2444354 0.2680884 0.2917415 0.3153945 0.3390476
##  [8] 0.3627006 0.3863537 0.4100067
```

---
background-image: url(figs/intermacs_model_table.png)
background-position: 50% 70%
background-size: 80%

---
background-image: url(figs/intermacs_model_table_edited.jpg)
background-position: 50% 70%
background-size: 80%

---
background-image: url(figs/intermacs_logo.jpg)
background-position: 97% 3%
background-size: 15%

### ML workflow for INTERMACS




`frm_1` is a model formula comprising variables listed in the early hazard phase.


```r
# expert model formula 

frm_1 &lt;- Surv(time, status) ~ 
  age2 + # age squared 
  ccs +  # critical cardiogenic shock
  m0_cv_pres + # central venous pressure
  m0_bili_total_mg_dl + # bilirubin
  m0_lvedd + # left ventricular end-diastolic dimension
  m0_intervention_48_hrs_dialysis + # prior to surgery
  m0_bun_mg_dl + # blood urea nitrogen
  m0_albumin_g_dl + # urinary albumin
  m0_prev_cardiac_oper_none # no previous heart operation
```

---

background-image: url(hex_stickers/PNG/pipe.png)
background-position: 95% 2.5%
background-size: 13%




ML workflow using `%&gt;%` (read as **then**).

1. Fit the Cox Proportional Hazards (`coxph`) model, **then**

2. using the `coxph` model, predict survival probabilities in the `.validation` data, **then**

3. using the predicted survival probabilities at all of the `btimes`, compute the IBS.


```r
ibs_cph_1 &lt;- coxph(frm_1, data = .derivation, x = TRUE) %&gt;% 
  predictSurvProb(newdata = .validation, times = btimes) %&gt;% 
  ibs_scale(newdata = .validation, eval_times = btimes)

ibs_cph_1
```

```
## [1] 0.03848166
```

---

background-image: url(hex_stickers/PNG/pipe.png)
background-position: 95% 2.5%
background-size: 13%


What if we use week 1 follow-up data instead of pre-implant data?


```r
frm_2 &lt;- Surv(time, status) ~ 
  age2 + # age squared 
  ccs +  # critical cardiogenic shock
  m0_25_cv_pres + # central venous pressure
  m0_25_bili_total_mg_dl + # bilirubin
  m0_25_lvedd + # left ventricular end-diastolic dimension
  m0_intervention_48_hrs_dialysis + # prior to surgery
  m0_25_bun_mg_dl + # blood urea nitrogen
  m0_25_albumin_g_dl + # urinary albumin
  m0_prev_cardiac_oper_none # no previous heart operation


ibs_cph_2 &lt;- coxph(frm_2, data = .derivation, x = TRUE) %&gt;% 
  predictSurvProb(newdata = .validation, times = btimes) %&gt;% 
  ibs_scale(newdata = .validation, eval_times = btimes)

ibs_cph_2
```

```
## [1] 0.08318288
```

---
background-image: url(hex_stickers/PNG/purrr.png)
background-position: 95% 2.5%
background-size: 13%

We can clean this iterative code up with `purrr::map()`


```r
ibs_vals &lt;- list(
  `Pre-implant data` = frm_1,
  `Week 1 follow-up data` = frm_2
) %&gt;% 
  map_dbl(
    .f = ~ coxph(.x, data = .derivation, x = TRUE) %&gt;% 
      predictSurvProb(newdata = .validation, times = btimes) %&gt;% 
      ibs_scale(newdata = .validation, eval_times = btimes)
  )

ibs_vals
```

```
##      Pre-implant data Week 1 follow-up data 
##            0.03848166            0.08318288
```

---

background-image: url(hex_stickers/PNG/knitr.png)
background-position: 95% 2.5%
background-size: 13%

And clean up the presentation with `knitr::kable()` + `kableExtra`

&lt;br/&gt;

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Table 2: Validation of three models using 2016-2017 INTERMACS data&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Model &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Scaled Brier &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; % Improvement &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Pre-implant data &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.038 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Week 1 follow-up data &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.083 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 116 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
layout: true
class: center, middle

---

# Just a second

---

## Should we really impute missing values using mean/mode?

---

## Should we be using proportional hazards?

---

## What happened to the other 500-ish variables?

---

## Is this the best (scaled &amp; integrated) Brier score we can get?

---
layout: false
class: center, middle

# Core principal 3:

# Modeling algorithms

---
layout: true 
background-image: url(hex_stickers/PNG/tidymodels.png)
background-position: 95% 2.5%
background-size: 13%

# Core Principles of ML

### Modeling algorithms

**Definition**

Think about the steps taken to develop a prediction function, `\(\widehat{f}\)`. 

That's the ad-hoc modeling algorithm definition. More formally, there are phases: 

---

- **Data pre-processing**

    + Check validity of the data, 
    + handle missing values (e.g., impute / discard), 
    + handle factor levels (e.g., collapse / combine), 
    + scale/center continuous variables, 
    + create new predictor variables, 
    + handle outliers, 
    + and much more.
    
---

- **Model development**

    + Select modeling procedure,
    + Select tuning parameters,
    + Select predictors,
    + Fit the model(s)

    
---
layout: false
background-image: url(hex_stickers/PNG/tidymodels.png)
background-position: 95% 2.5%
background-size: 13%

# Core Principles of ML

### Modeling algorithms

Let's focus on the **model development** phase. Our initial approach used 

- a traditional statistical modeling procedure (`coxph`)

- no tuning parameters

- nine predictor variables selected by experts

Let's evaluate an alternate approach that uses 

- a stepwise implementation of `coxph`

- one tuning parameter indicating the max number of steps: nine

- nine (or less) predictor variables selected from about 500 candidates.

---
background-image: url(figs/intermacs_logo.jpg)
background-position: 97% 3%
background-size: 15%

# Core Principles of ML

### Stepwise selection with INTERMACS


```r
# pull x variables out of recipe
xnames &lt;- reci$var_info %&gt;% 
  filter(role == 'predictor') %&gt;% 
  pull(variable)

# create full model formula (i.e., all predictors)
full &lt;- glue(
  "Surv(time, status) ~ {glue_collapse(xnames, sep = '+')}"
)
```

---
background-image: url(figs/intermacs_logo.jpg)
background-position: 97% 3%
background-size: 15%

# Core Principles of ML

### Stepwise selection with INTERMACS


```r
# fit stepwise model
step_mdl &lt;- stepAIC(
  object = coxph(Surv(time, status) ~ 1, data = .derivation),
  scope = as.formula(full),
  direction = 'both',
  steps = 9,
  trace = 0
)

# Extract the formula that was identified 
frm_3 &lt;- step_mdl$formula
```

---
background-image: url(hex_stickers/PNG/purrr.png)
background-position: 95% 2.5%
background-size: 13%

# Core Principles of ML

### Stepwise selection with INTERMACS

Now we just add `frm_3` to our `purrr::map()` code.




```r
ibs_vals &lt;- list(
  `Pre-implant data` = frm_1,
  `Week 1 follow-up data` = frm_2,
  `Forward selection (pre-implant + week 1)` = frm_3
) %&gt;% 
  map_dbl(
    .f = ~ coxph(.x, data = .derivation, x = TRUE) %&gt;% 
      predictSurvProb(newdata = .validation, times = btimes) %&gt;% 
      ibs_scale(newdata = .validation, eval_times = btimes)
  )
```

---

# Core Principles of ML

### Variables selected

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2"&gt;&lt;div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; "&gt;Expert formulas&lt;/div&gt;&lt;/th&gt;
&lt;th style="border-bottom:hidden" colspan="1"&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:center;"&gt; Pre-implant data &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Week 1 follow-up data &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Forward selection &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; age2 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; age2 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_platelet_x10_3_ul &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; ccs &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; ccs &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_intub &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; m0_cv_pres &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_cv_pres &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_bun_mg_dl &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; m0_bili_total_mg_dl &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_bili_total_mg_dl &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_dialysis &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; m0_lvedd &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_lvedd &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_age_deident &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; m0_intervention_48_hrs_dialysis &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_intervention_48_hrs_dialysis &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_bili_total_mg_dl &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; m0_bun_mg_dl &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_bun_mg_dl &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_warfarin &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; m0_albumin_g_dl &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_albumin_g_dl &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_sgot_ast &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; m0_prev_cardiac_oper_none &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_prev_cardiac_oper_none &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; m0_25_cv_pres &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
background-image: url(hex_stickers/PNG/knitr.png)
background-position: 95% 2.5%
background-size: 13%

# Core Principles of ML

### Stepwise selection with INTERMACS

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Table 3: Validation of three models using 2016-2017 INTERMACS data&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Model &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Scaled Brier &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; % Improvement &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Pre-implant data &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.038 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Week 1 follow-up data &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.083 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 116 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Forward selection (pre-implant + week 1) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.152 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 296 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;br/&gt;

*wow, that's not bad*.

&lt;br/&gt;

**Question** How do we know if it was optimal to take nine steps?

---
class: center, middle

# Core principal 4:

# Resampling

---
layout: true
background-image: url(hex_stickers/PNG/rsample.png)
background-position: 95% 2.5%
background-size: 13%

# Core Principles of ML

---

### Resampling

**Why is this a core principle?**

Resampling allows analysts to simulate the development and external validation of a prediction equation using a specific modeling algorithm.

**What is resampling?**

We will talk about two types of resampling here:

1. *Monte-Carlo cross-validation*: Repeatedly split an initial dataset into training/testing sets

2. *K-fold cross-validation*: Partitioning an initial dataset into folds, then letting each fold serve as a testing set while the remaining folds form a training set.

Note: CV = cross-validation from here on out.

---

### Monte-Carlo CV

Randomly select a set of training data (blue rows) from the derivation data.

![](index_files/figure-html/unnamed-chunk-72-1.png)&lt;!-- --&gt;

---

### Monte-Carlo CV

Use the training data to develop one prediction rule per modeling algorithm. 

![](index_files/figure-html/unnamed-chunk-73-1.png)&lt;!-- --&gt;

---

### Monte-Carlo CV

Apply each prediction rule to the testing data, creating a set of predictions for each. 

![](index_files/figure-html/unnamed-chunk-74-1.png)&lt;!-- --&gt;

---

### Monte-Carlo CV

Evaluate each set of predictions

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:center;"&gt; P(Mortality) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Mortality &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Difference &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Squared Difference &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 0.51 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -0.49 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.2401 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 0.54 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.54 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.2916 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 0.53 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.53 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.2809 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 0.60 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.60 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.3600 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

`$$\text{Brier Score} = \frac{1}{N} \sum_{i=1}^N (\widehat{f}(x_i)-y_i)^2 = \frac{0.24 + 0.29 + 0.28 + 0.36}{4} = 0.29$$`

---
layout: false
class: middle, center

# Resampling diagrams

---
class: middle, center

&lt;img src="index_files/figure-html/unnamed-chunk-76-1.png" style="display: block; margin: auto;" /&gt;

---
background-image: url(figs/kfold_cv.png)
background-position: 50% 50%
background-size: 100%

---
layout: true
background-image: url(hex_stickers/PNG/rsample.png)
background-position: 95% 2.5%
background-size: 13%

# Computers and k-fold CV

---

Remember how we selected 9 variables for our first stepwise model?


```r
# fit stepwise model
step_mdl &lt;- stepAIC(
  object = coxph(Surv(time, status) ~ 1, data = .derivation),
  scope = as.formula(full),
  direction = 'both',
  steps = 9,
  trace = 0
)

# Extract the formula that was identified 
frm_3 &lt;- step_mdl$formula
```


---


**Question** How long did that take?

- There are ~530 candidate predictor variables, so we fit ~530 models in each step. (Why?)

- Suppose it takes about 0.11 seconds to fit each model.

- `\(\rightarrow\)` it takes roughly 58 seconds (~1 minute) to complete one step.

- `\(\rightarrow\)` It takes roughly 530 minutes (~9 hours) to complete 530 steps. 

---

**Question** How long would it take to find the best step number using 10-fold CV?

- Divide the derivation data into 10 folds (~1 second)

- Run through all the steps using folds 1-9 as training data, and validating on fold 10 (~ 9 hours)

- Repeat 9 more times validating on folds 9, 8, ..., and 1 (~ 81 hours)

**Answer** About 90 hours, which is almost 4 days.

--

**What if we could do this in under 5 minutes?**

---
layout: true
background-image: url(figs/esl.jpg)
background-position: 96% 4%
background-size: 18%

# Elastic net regression

---

### What is it?

An extension of linear regression that penalizes regression solutions with larger coefficients. Shrinking coefficients to zero reduces the variance of the model and increases the bias.

--

### Why use it?

- Variable selection is completed during the fitting process

- `\(N &lt; p\)` is no problem (N = number of rows, p = number of columns)

- The `glmnet` package is **fast** and fits all kinds of elastic net models. 

---

**How to use it**

The `glmnet` R package is **fast** but not intuitive.

- Data need to be coerced into a matrix with one-hot encoded categorical columns. 


```r
library(xgboost.surv) # for cat_yyy functions

train_x &lt;- .derivation %&gt;% 
  select(-c(time, status)) %&gt;% 
  cat_spread() %&gt;% 
  as.matrix()

train_y &lt;- .derivation %&gt;% 
  select(time, status) %&gt;% 
  as.matrix()

test_x &lt;- .validation %&gt;% 
  select(-c(time, status)) %&gt;% 
  cat_transfer(.derivation) %&gt;% 
  cat_spread() %&gt;% 
  as.matrix()
```

---

**How to use it**

The `glmnet` R package is **fast** but not intuitive.

- A parameter called `alpha` needs to be specified explicitly. Another parameter called `lambda` is determined using cross-validation. 


```r
library(glmnet)

start &lt;- Sys.time()
net_cv &lt;- cv.glmnet(
  x = train_x, 
  y = train_y, 
  family = 'cox', 
  alpha = 0.10
)
stop &lt;- Sys.time()

print(stop - start)
```

```
## Time difference of 3.655493 mins
```

---

**How to use it**

The `glmnet` R package is **fast** but not intuitive.

- `plot(net_cv)` require a lot of explanation.


![](index_files/figure-html/unnamed-chunk-80-1.png)&lt;!-- --&gt;


---

**How to use it**

The `glmnet` R package is **fast** but not intuitive.

- Computing predicted survival probability is done using `c060`.

- The authors of `c060` are still developing the functions to do this. 


```r
net_prd  &lt;- c060:::fit.glmnet(
  response = train_y, 
  x = train_x, 
  family = 'cox',
  alpha = 0.10,
  cplx = net_cv$lambda.min
) %&gt;% 
  c060:::predictProb.glmnet(
    x = test_x,
    times = btimes,
    response = train_y,
    complexity = net_cv$lambda.min
  )
```

---

The `glmnet` R package is **fast** but not intuitive.

- But it is worth it!


```r
net_ibs &lt;- ibs_scale(net_prd, newdata = .validation, btimes)

ibs_vals &lt;- c(ibs_vals,
  'Elastic regression (pre-implant + week 1)' = net_ibs)
```

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;Table 3: Validation of four models using 2016-2017 INTERMACS data&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Model &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Scaled Brier &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; % Improvement &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Pre-implant data &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.038 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Week 1 follow-up data &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.083 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 116 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Forward selection (pre-implant + week 1) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.152 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 296 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Elastic regression (pre-implant + week 1) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.162 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 322 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
